{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG With llama-index  + Milvus + Qwen - Part 2\n",
    "\n",
    "References\n",
    "\n",
    "- https://studio.nebius.com/\n",
    "- https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/\n",
    "- https://docs.llamaindex.ai/en/stable/api_reference/storage/vector_store/milvus/?h=milvusvectorstore#llama_index.vector_stores.milvus.MilvusVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) проверьте файл .env\n",
    "- 2) kernel - studio1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found NEBIUS_API_KEY in environment, using it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if os.getenv('NEBIUS_API_KEY'):\n",
    "    print (\"✅ Found NEBIUS_API_KEY in environment, using it\")\n",
    "else:\n",
    "    raise ValueError(\"❌ NEBIUS_API_KEY not found in environment. Please set it in .env file before running this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Setup Embedding Model\n",
    "\n",
    "We have a choice of local embedding model (fast) or running it on the cloud\n",
    "\n",
    "If running locally:\n",
    "- choose smaller models\n",
    "- less accuracy but faster\n",
    "\n",
    "If running on the cloud\n",
    "- We can run large models (billions of params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dasha/.conda/envs/studio-1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Option 1: Running embedding models on Nebius cloud\n",
    "#from llama_index.embeddings.nebius import NebiusEmbedding\n",
    "#EMBEDDING_MODEL = 'Qwen/Qwen3-Embedding-8B'  # 8B params\n",
    "#EMBEDDING_LENGTH = 4096  # Length of the embedding vector\n",
    "#Settings.embed_model = NebiusEmbedding(\n",
    "#                        model_name=EMBEDDING_MODEL,\n",
    "#                        embed_batch_size=50,  # Batch size for embedding (default is 10)\n",
    "#                        api_key=os.getenv(\"NEBIUS_API_KEY\") # if not specfified here, it will get taken from env variable\n",
    "#                       )\n",
    "\n",
    "# Option 2: Running embedding models locally\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    # model_name = 'sentence-transformers/all-MiniLM-L6-v2' # 23 M params\n",
    "    model_name = 'BAAI/bge-small-en-v1.5'  # 33M params\n",
    "    # model_name = 'Qwen/Qwen3-Embedding-0.6B'  # 600M params\n",
    "    # model_name = 'BAAI/bge-en-icl'  # 7B params\n",
    "    #model_name = 'intfloat/multilingual-e5-large-instruct'  # 560M params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance:  ./rag.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "DB_URI = './rag.db'  # For embedded instance\n",
    "COLLECTION_NAME = 'rag'\n",
    "\n",
    "milvus_client = MilvusClient(DB_URI)\n",
    "print (\"✅ Connected to Milvus instance: \", DB_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected Llama-index to Milvus instance:  ./rag.db\n"
     ]
    }
   ],
   "source": [
    "# connect to vector db\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "EMBEDDING_LENGTH = 384\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri = DB_URI ,\n",
    "    dim = EMBEDDING_LENGTH ,\n",
    "    collection_name = COLLECTION_NAME,\n",
    "    #overwrite=False\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print (\"✅ Connected Llama-index to Milvus instance: \", DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Load Document Index from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded index from vector db: ./rag.db\n",
      "CPU times: user 139 ms, sys: 16 ms, total: 155 ms\n",
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, storage_context=storage_context)\n",
    "\n",
    "print (\"✅ Loaded index from vector db:\", DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.llms.nebius import NebiusLLM\n",
    "#from llama_index.core import Settings\n",
    "#\n",
    "#Settings.llm = NebiusLLM(\n",
    "#                #model='openai/gpt-oss-120b',\n",
    "#                #model='Qwen/Qwen3-30B-A3B',\n",
    "#                model='deepseek-ai/DeepSeek-R1-0528',\n",
    "#                api_key=os.getenv(\"NEBIUS_API_KEY\") # if not specfified, it will get taken from env variable\n",
    "#    )\n",
    "# no access 403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.llms.ollama import Ollama\n",
    "#from llama_index.core import Settings\n",
    "#\n",
    "#Settings.llm = Ollama(\n",
    "#    model=\"llama3.1\",  #\"mistral\", \"qwen2.5:7b\"\n",
    "#    request_timeout=120.0\n",
    "#)\n",
    "#\n",
    "#\n",
    "#model requires more system memory (29.7 GiB) than is available (14.1 GiB) (status code: 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "import sacremoses\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"microsoft/BioGPT-Large\",\n",
    "    tokenizer_name=\"microsoft/BioGPT-Large\",\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=512,\n",
    "    context_window=2048,\n",
    "    generate_kwargs={\n",
    "        \"temperature\": 0.3,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# это чот пиздец долго думает\n",
    "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "#from llama_index.core import Settings\n",
    "#\n",
    "#\n",
    "#Settings.llm = HuggingFaceLLM(\n",
    "#    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#    device_map=\"auto\",\n",
    "#    max_new_tokens=512,\n",
    "#    context_window=4096,\n",
    "#    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True}\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Empty Response\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"What was Uber's revenue for 2020?\")\n",
    "print(\"Response:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure the model uses context\n",
    "\n",
    "Let's ask a generic factual question \"When was the moon landing\".\n",
    "\n",
    "Now the model should know this generic factual answer.\n",
    "\n",
    "But since we are querying documents, we want to the model to find answers from within the documents.\n",
    "\n",
    "It should come back with something like \"provided context does not have information about moon landing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "res = query_engine.query(\"When was the moon landing?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studio-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
