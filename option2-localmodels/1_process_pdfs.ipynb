{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG With llama-index  + Milvus + Qwen - Part 1\n",
    "\n",
    "References\n",
    "\n",
    "- https://studio.nebius.com/\n",
    "- https://docs.llamaindex.ai/en/stable/examples/vector_stores/MilvusIndexDemo/\n",
    "- https://docs.llamaindex.ai/en/stable/api_reference/storage/vector_store/milvus/?h=milvusvectorstore#llama_index.vector_stores.milvus.MilvusVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) убедитесь, что есть файл .env\n",
    "- 2) kernel - studio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found NEBIUS_API_KEY in environment, using it\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if os.getenv('NEBIUS_API_KEY'):\n",
    "    print (\"✅ Found NEBIUS_API_KEY in environment, using it\")\n",
    "else:\n",
    "    raise ValueError(\"❌ NEBIUS_API_KEY not found in environment. Please set it in .env file before running this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Read documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 545 chunks\n",
      "CPU times: user 18.3 s, sys: 265 ms, total: 18.6 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import pprint\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir = '../data/10k',\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(documents)} chunks\")\n",
    "\n",
    "# print(\"Document [0].doc_id:\", documents[0].doc_id)\n",
    "# pprint.pprint (documents[0], indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Setup Embedding Model\n",
    "\n",
    "We have a choice of local embedding model (fast) or running it on the cloud\n",
    "\n",
    "If running locally:\n",
    "- choose smaller models\n",
    "- less accuracy but faster\n",
    "\n",
    "If running on the cloud\n",
    "- We can run large models (billions of params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dasha/.conda/envs/studio-1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-16 15:06:26,561 - INFO - Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "2025-10-16 15:06:34,973 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_index.core import Settings\n",
    "\n",
    "## These work excellently for most RAG applications\n",
    "#\"BAAI/bge-small-en-v1.5\"        # 384 dimensions  - Fast & efficient\n",
    "#\"BAAI/bge-base-en-v1.5\"         # 768 dimensions  - Balanced\n",
    "#\"BAAI/bge-large-en-v1.5\"        # 1024 dimensions - High quality\n",
    "#\"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions - Very popular\n",
    "\n",
    "\n",
    "# Option 1: Running embedding models on Nebius cloud\n",
    "#from llama_index.embeddings.nebius import NebiusEmbedding\n",
    "#EMBEDDING_MODEL = 'Qwen/Qwen3-Embedding-8B'  # 8B params\n",
    "#EMBEDDING_LENGTH = 384 \n",
    "#Settings.embed_model = NebiusEmbedding(\n",
    "#                        model_name=EMBEDDING_MODEL,\n",
    "#                        embed_batch_size=50,  # Batch size for embedding (default is 10)\n",
    "#                        api_key=os.getenv(\"NEBIUS_API_KEY\") # if not specfified here, it will get taken from env variable\n",
    "#                       )\n",
    "\n",
    "#Option 2: Running embedding models locally\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    # model_name = 'sentence-transformers/all-MiniLM-L6-v2' # 23 M params\n",
    "    model_name = 'BAAI/bge-small-en-v1.5'  # 33M params\n",
    "    # model_name = 'Qwen/Qwen3-Embedding-0.6B'  # 600M params\n",
    "    # model_name = 'BAAI/bge-en-icl'  # 7B params\n",
    "    #model_name = 'intfloat/multilingual-e5-large-instruct'  # 560M params\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance:  ./rag.db\n",
      "✅ Cleared collection : rag\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "DB_URI = './rag.db'  # For embedded instance\n",
    "COLLECTION_NAME = 'rag'\n",
    "\n",
    "milvus_client = MilvusClient(DB_URI)\n",
    "print (\"✅ Connected to Milvus instance: \", DB_URI)\n",
    "\n",
    "# if we already have a collection, clear it first\n",
    "if milvus_client.has_collection(collection_name = COLLECTION_NAME):\n",
    "    milvus_client.drop_collection(collection_name = COLLECTION_NAME)\n",
    "    print ('✅ Cleared collection :', COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected Llama-index to Milvus instance:  ./rag.db\n",
      "CPU times: user 15.9 ms, sys: 1.15 ms, total: 17.1 ms\n",
      "Wall time: 555 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# connect to vector db\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "\n",
    "EMBEDDING_LENGTH = 384\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri = DB_URI ,\n",
    "    dim = EMBEDDING_LENGTH ,\n",
    "    collection_name = COLLECTION_NAME,\n",
    "    overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print (\"✅ Connected Llama-index to Milvus instance: \", DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Create Index and Save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=512,      # ⬇️ Reduce from default (usually 1024)\n",
    "    chunk_overlap=50,    # Small overlap for context\n",
    "    separator=\" \",       # Split by spaces/words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then use it when creating your index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    storage_context=storage_context,\n",
    "    transformations=[text_splitter] \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studio-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
